#!/bin/bash
#SBATCH -p short             # debug - for testing ; short - up to 4 hours ; batch - up to 5 days #
#SBATCH --time=04:00:00      # dd-hh:mm:ss
#SBATCH --mem-per-cpu=3000   # 1000 - 3500 nromally, but can be even higher
#SBATCH -J pre-nok           # name 
#SBATCH -o sbatch-csl-%j.out # where the outputs & errors are written
#SBATCH --constraint=csl     # require Haswell CPUs with 24 cores per node (ivb, hsw, vdw, skl }= avx;  hsw, vdw, skl }= avx2 )  
#SBATCH -N 1
#SBATCH -n 8                # n processes to run (N x 24 = n); max 192 ; max debug 48

module purge
module load cp2k/6.1-openmpi-scalapack-popt anaconda/2020-04-tf2
export OMP_NUM_THREADS=1
ulimit -s unlimited

#source /home/krejcio1/.bashrc
eval "$(conda shell.bash hook)"
conda activate KPFM1
progdir=/home/krejcio1/wrkdir/Programs/ana-20.04tf2_py3.7.7

export PYTHONPATH=$PYTHONPATH:$progdir/KPFM_sim3:$progdir/CP2k_mtools3:$progdir/DFT_gridIO3


# for run_task.py #
#if len(sys.argv) == 4:
#    task_db_file = sys.argv[1]
#    project_path = sys.argv[2]
#    slurm_id = int(sys.argv[3])
#    task_type_constraint = None
#    task_state_constraint = default_state_constraint
# ############### #

task_db_file=task/test1.db
project_path=./
slurm_id=$SLURM_JOB_ID

mkdir glob_res

python3 preopt_vertical_along_y_sample_first.py

./cp_opt.sh

#python3 plan_descend_tip_task.py $task_db_file ./result/test1.db ./glob_res/test_in.db

#python3 run_task.py $task_db_file $project_path $slurm_id


echo "Done, done"

#echo "arguments to srun" $cp2k_bin $output_file $input_file
#echo "Name of the partition in which the job is running: $SLURM_JOB_PARTITION"
#echo "Name of the node running the job script: $SLURMD_NODENAME"
#echo "     The ID of the job allocation: $SLURM_JOB_ID"
#echo "     List of nodes allocated to the job: $SLURM_JOB_NODELIST"

